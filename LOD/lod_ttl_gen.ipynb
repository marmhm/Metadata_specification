{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa72ae1-4001-4cd1-bf7a-01c25e9937f4",
   "metadata": {},
   "source": [
    "# Convert json to turtle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d66dffc-22db-43f9-8cb7-cd6a90c331f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTL files saved in the folder: ttl_files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import json\n",
    "from urllib.parse import unquote_plus,urlparse\n",
    "from langdetect import detect, LangDetectException\n",
    "import re\n",
    "from urllib.parse import urlparse, urlunparse, quote\n",
    "\n",
    "def clean_integer(value):\n",
    "    # Remove commas, non-numeric characters, and trim whitespace\n",
    "    cleaned_value = re.sub(r\"[^\\d]\", \"\", str(value).strip())\n",
    "\n",
    "    # Ensure we don't return an empty string (which would cause errors)\n",
    "    return cleaned_value if cleaned_value else \"0\"  # Default to \"0\" if invalid\n",
    "\n",
    "\n",
    "def clean_url(url):\n",
    "    if not url or not isinstance(url, str) or url.strip() == \"\":\n",
    "        return \"\"  # Return empty string if URL is None or empty\n",
    "    \n",
    "    url = url.strip()  # Remove leading/trailing spaces\n",
    "    \n",
    "    # If multiple URLs are mistakenly concatenated, keep only the first valid one\n",
    "    url_parts = url.split()  # Split by spaces\n",
    "    url = url_parts[0] if url_parts else \"\"  # Safely get the first part\n",
    "\n",
    "    parsed_url = urlparse(url)\n",
    "\n",
    "    # Fix invalid file URLs (encode spaces)\n",
    "    if parsed_url.scheme == \"file\":\n",
    "        url = quote(url, safe=\":/\")\n",
    "\n",
    "    # Remove invalid characters\n",
    "    allowed_chars = r\"[^A-Za-z0-9\\-._~:/?#\\[\\]@!$&'()*+,;=%]\"\n",
    "    \n",
    "    cleaned_scheme = re.sub(allowed_chars, \"\", parsed_url.scheme)\n",
    "    cleaned_netloc = re.sub(allowed_chars, \"\", parsed_url.netloc)\n",
    "    cleaned_path = quote(parsed_url.path.strip(), safe=\"/:\")  # Encode spaces\n",
    "    cleaned_params = re.sub(allowed_chars, \"\", parsed_url.params)\n",
    "    cleaned_query = quote(parsed_url.query, safe=\"&=\")  # Encode query\n",
    "    cleaned_fragment = quote(parsed_url.fragment, safe=\"\")  # Encode fragment\n",
    "\n",
    "    # Reconstruct the cleaned URL\n",
    "    cleaned_url = urlunparse((\n",
    "        cleaned_scheme, \n",
    "        cleaned_netloc, \n",
    "        cleaned_path, \n",
    "        cleaned_params, \n",
    "        cleaned_query, \n",
    "        cleaned_fragment\n",
    "    ))\n",
    "\n",
    "    return cleaned_url   # Return cleaned URL or empty string\n",
    "\n",
    "\n",
    "\n",
    "# Define the prefixes as before, now including the vcard prefix\n",
    "prefixes = \"\"\"\n",
    "@prefix : <http://example.org/> . \n",
    "@prefix dct: <http://purl.org/dc/terms/> . \n",
    "@prefix void: <http://rdfs.org/ns/void#> . \n",
    "@prefix dcat: <http://www.w3.org/ns/dcat#> . \n",
    "@prefix foaf: <http://xmlns.com/foaf/0.1/> . \n",
    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
    "@prefix schema: <http://schema.org/> .\n",
    "@prefix adms:<http://www.w3.org/TR/vocab-adms/>.\n",
    "@prefix prov: <http://www.w3.org/ns/prov#> .\n",
    "@prefix vcard: <http://www.w3.org/2006/vcard/ns#> .  # vCard for contact details\n",
    "\"\"\"\n",
    "ttl_data = \"\"\n",
    "\n",
    "def fix_email(email):\n",
    "    \"\"\" Fix common issues in email, e.g., replacing 'at' with '@', removing spaces and quotes.\"\"\"\n",
    "    if email:\n",
    "        # Remove spaces\n",
    "        email = email.replace(\" \", \"\")\n",
    "        # Remove any quotation marks (either single or double)\n",
    "        email = email.replace(\"\\\"\", \"\").replace(\"'\", \"\")\n",
    "        # Replace \"at\" with \"@\" if it's present\n",
    "        email = email.replace(\"at\", \"@\")\n",
    "    return email\n",
    "\n",
    "def fix_name(name):\n",
    "   \n",
    "    if name:\n",
    "        # Remove any quotation marks (either single or double)\n",
    "        name = name.replace(\"\\\"\", \"\").replace(\"'\", \"\").replace(\" - \", \"\").replace(\" \", \"_\")\n",
    "    return name\n",
    "\n",
    "\n",
    "def json_to_ttl(json_data, dataset_id, sanitized_id):\n",
    "    ttl_data = f\":{sanitized_id} a dcat:Dataset ;\\n\"\n",
    "    linkset_data = \"\"  # Separate storage for links\n",
    "    dataset = json_data[dataset_id]\n",
    "    owner_details = \"\"\n",
    "    contact_details = \"\"\n",
    "    # Iterate over the dataset fields and convert them to TTL format\n",
    "    for key, value in dataset.items():\n",
    "        if not value:  # Skip keys with empty or None values\n",
    "            continue\n",
    "            \n",
    "        if key == \"links\":  # Skip links, we will process them separately\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # Handle each key based on its specific structure and add it to TTL\n",
    "        if key == \"title\":\n",
    "            value= fix_name(value)\n",
    "            ttl_data += f\"    dct:title \\\"{value}\\\" ;\\n\"\n",
    "        elif key == \"description\" and isinstance(value, dict):\n",
    "            for lang, desc in value.items():\n",
    "                # Multiline string for description\n",
    "                ttl_data += f\"    dct:description \\\"\\\"\\\"\\n{desc}\\n\\\"\\\"\\\"@{lang} ;\\n\\n\"  # Add language tag to description\n",
    "        elif key == \"full_download\" and isinstance(value, list):\n",
    "            for item in value:\n",
    "                ttl_data += \"    dcat:distribution [\\n\"\n",
    "                if isinstance(item, dict):\n",
    "                    for subkey, subvalue in item.items():\n",
    "                        if subkey == \"access_url\":\n",
    "                            if isinstance(subvalue, str):\n",
    "                                subvalue = subvalue.strip()  # Remove spaces\n",
    "                                if subvalue and subvalue.startswith((\"http://\", \"https://\")):\n",
    "                                    subvalue = clean_url(subvalue)\n",
    "                                    ttl_data += f\"        dcat:accessURL <{subvalue}> ;\\n\"\n",
    "                                else:\n",
    "                                    continue  # ✅ Completely skip empty or invalid URLs\n",
    "                        elif subkey == \"download_url\":\n",
    "                            if isinstance(subvalue, str):\n",
    "                                subvalue = subvalue.strip()  # Remove spaces\n",
    "                                if subvalue and subvalue.startswith((\"http://\", \"https://\")):\n",
    "                                    subvalue = clean_url(subvalue)\n",
    "                                    ttl_data += f\"        dcat:downloadURL <{subvalue}> ;\\n\"\n",
    "                                else:\n",
    "                                    continue  # ✅ Completely skip empty or invalid URLs\n",
    "                        elif subkey == \"title\":\n",
    "                            subvalue = fix_name(subvalue)\n",
    "                            ttl_data += f\"        dct:title \\\"{subvalue}\\\" ;\\n\"\n",
    "                        elif subkey == \"description\":\n",
    "                            ttl_data += f\"        dct:description \\\"\\\"\\\"\\n{subvalue}\\n\\\"\\\"\\\" ;\\n\"\n",
    "                        elif subkey == \"media_type\":\n",
    "                            subvalue = fix_name(subvalue)\n",
    "                            ttl_data += f\"        dcat:mediaType \\\"{subvalue}\\\" ;\\n\"\n",
    "                        elif subkey == \"status\":\n",
    "                            ttl_data += f\"        adms:status \\\"\\\"\\\"\\n{subvalue}\\n\\\"\\\"\\\" ;\\n\"\n",
    "                        elif subkey == \"mirror\":\n",
    "                            ttl_data += f\"        dct:mirror \\\"{subvalue}\\\" ;\\n\"\n",
    "                        elif subkey == \"_id\":\n",
    "                            ttl_data += f\"        dct:identifier \\\"{subvalue}\\\" ;\\n\"\n",
    "                ttl_data += \"    ] ;\\n\"\n",
    "        elif key == \"other_download\" and isinstance(value, list):\n",
    "            for item in value:\n",
    "                ttl_data += \"    dcat:distribution [\\n\"  # Start a new blank node for each download entry\n",
    "                if isinstance(item, dict):\n",
    "                    for subkey, subvalue in item.items():\n",
    "                        if subkey == \"access_url\":\n",
    "                            if isinstance(subvalue, str):\n",
    "                                subvalue = subvalue.strip()  # Remove spaces\n",
    "                                if subvalue and subvalue.startswith((\"http://\", \"https://\")):\n",
    "                                    subvalue = clean_url(subvalue)\n",
    "                                    ttl_data += f\"        dcat:accessURL <{subvalue}> ;\\n\"\n",
    "                                else:\n",
    "                                    continue  # ✅ Completely skip empty or invalid URLs                        elif subkey == \"title\":\n",
    "                        elif subkey == \"title\":\n",
    "                                subvalue = fix_name(subvalue) \n",
    "                                ttl_data += f\"        dct:title \\\"{subvalue}\\\" ;\\n\"\n",
    "                        elif subkey == \"description\":\n",
    "                            ttl_data += f\"        dct:description \\\"\\\"\\\"\\n{subvalue}\\n\\\"\\\"\\\" ;\\n\"\n",
    "                        elif subkey == \"media_type\":\n",
    "                            subvalue = fix_name(subvalue) \n",
    "                            ttl_data += f\"        dcat:mediaType \\\"{subvalue}\\\" ;\\n\"\n",
    "                        elif subkey == \"status\":\n",
    "                            ttl_data += f\"        adms:status \\\"\\\"\\\"\\n{subvalue}\\n\\\"\\\"\\\" ;\\n\"\n",
    "                        elif subkey == \"mirror\":\n",
    "                            ttl_data += f\"        dct:mirror \\\"{subvalue}\\\" ;\\n\"\n",
    "                        elif subkey == \"_id\":\n",
    "                            ttl_data += f\"        dct:identifier \\\"{subvalue}\\\" ;\\n\"\n",
    "                ttl_data += \"    ] ;\\n\"  # End the blank node for the current item\n",
    "                \n",
    "        elif key == \"sparql\" and isinstance(value, list):\n",
    "            for item in value:\n",
    "                ttl_data += \"    void:sparqlEndpoint [\\n\"  # Start a new blank node for each download entry\n",
    "                if isinstance(item, dict):\n",
    "                    for subkey, subvalue in item.items():\n",
    "                        if subkey == \"access_url\":\n",
    "                            if isinstance(subvalue, str):\n",
    "                                subvalue = subvalue.strip()  # Remove spaces\n",
    "                                if subvalue and subvalue.startswith((\"http://\", \"https://\")):\n",
    "                                    subvalue = clean_url(subvalue)\n",
    "                                    ttl_data += f\"        dcat:accessURL <{subvalue}> ;\\n\"\n",
    "                                else:\n",
    "                                    continue  # ✅ Completely skip empty or invalid URLs                        elif subkey == \"title\":\n",
    "                        elif subkey == \"title\":\n",
    "                            subvalue = fix_name(subvalue)\n",
    "                            ttl_data += f\"        dct:title \\\"{subvalue}\\\" ;\\n\"\n",
    "                        elif subkey == \"description\":\n",
    "                            ttl_data += f\"        dcat:endpointDescription \\\"\\\"\\\"\\n{subvalue}\\n\\\"\\\"\\\" ;\\n\"\n",
    "                        elif subkey == \"status\":\n",
    "                            ttl_data += f\"        adms:status \\\"\\\"\\\"\\n{subvalue}\\n\\\"\\\"\\\" ;\\n\"\n",
    "                        elif subkey == \"media_type\":\n",
    "                            subvalue = fix_name(subvalue) \n",
    "                            ttl_data += f\"        dcat:mediaType \\\"{subvalue}\\\" ;\\n\"\n",
    "                        elif subkey == \"mirror\":\n",
    "                            ttl_data += f\"        dct:mirror \\\"{subvalue}\\\" ;\\n\"\n",
    "                        elif subkey == \"_id\":\n",
    "                            ttl_data += f\"        dct:identifier \\\"{subvalue}\\\" ;\\n\"\n",
    "                ttl_data += \"    ] ;\\n\"  # End the blank node for the current item\n",
    "        elif key == \"example\" and isinstance(value, list):\n",
    "            for item in value:\n",
    "                ttl_data += \"    void:exampleResource [\\n\"\n",
    "                if isinstance(item, dict):\n",
    "                    for subkey, subvalue in item.items():\n",
    "                        if subkey == \"access_url\":\n",
    "                            if isinstance(subvalue, str):\n",
    "                                subvalue = subvalue.strip()  # Remove spaces\n",
    "                                if subvalue and subvalue.startswith((\"http://\", \"https://\")):\n",
    "                                    subvalue = clean_url(subvalue)\n",
    "                                    ttl_data += f\"        dcat:accessURL <{subvalue}> ;\\n\"\n",
    "                                else:\n",
    "                                    continue  # ✅ Completely skip empty or invalid URLs                        elif subkey == \"title\":\n",
    "                        elif subkey == \"title\":\n",
    "                            subvalue = fix_name(subvalue)\n",
    "                            ttl_data += f\"        dct:title \\\"{subvalue}\\\" ;\\n\"\n",
    "                        elif subkey == \"description\":\n",
    "                            ttl_data += f\"        dct:description \\\"\\\"\\\"\\n{subvalue}\\n\\\"\\\"\\\" ;\\n\"\n",
    "                        elif subkey == \"media_type\":\n",
    "                            subvalue= fix_name(subvalue)\n",
    "                            ttl_data += f\"        dcat:mediaType \\\"{subvalue}\\\" ;\\n\"\n",
    "                        elif subkey == \"status\":\n",
    "                            ttl_data += f\"        adms:status \\\"\\\"\\\"\\n{subvalue}\\n\\\"\\\"\\\" ;\\n\"\n",
    "                ttl_data += \"    ] ;\\n\"\n",
    "        elif key == \"contact_point\" and isinstance(value, dict):  \n",
    "            ttl_data += \"    prov:qualifiedAttribution [\\n\"\n",
    "            ttl_data += \"        prov:agent :contactAgent ;\\n\"\n",
    "            ttl_data += \"        dcat:hadRole :contact_point ;\\n\"\n",
    "            ttl_data += \"    ] ;\\n\"\n",
    "\n",
    "            # Store contact details separately to append at the end\n",
    "            contact_details = \"\\n:contactAgent a prov:Agent ;\\n\"\n",
    "\n",
    "            # Handle multiple names\n",
    "            names = value.get(\"name\")\n",
    "            if names:\n",
    "                if isinstance(names, str):  # Convert single name to a list for uniform processing\n",
    "                    names = [names]\n",
    "                for name in names:\n",
    "                    contact_details += \"     foaf:name \\\"{}\\\"^^xsd:string ;\\n\".format(fix_name(name))\n",
    "\n",
    "            # Handle email\n",
    "            email = fix_email(value.get(\"email\", \"\"))\n",
    "            if email:\n",
    "                contact_details += \"     foaf:mbox <mailto:{}> ; \\n\".format(email)\n",
    "            contact_details += \" .\\n\\n\"\n",
    "            # Ensure correct syntax: If no names or emails exist, don't write this block\n",
    "            if \"foaf:name\" not in contact_details and \"foaf:mbox\" not in contact_details:\n",
    "                contact_details = \"\"  # Remove empty block\n",
    "\n",
    "        elif key == \"owner\" and isinstance(value, dict):  \n",
    "            ttl_data += \"    prov:qualifiedAttribution [\\n\"\n",
    "            ttl_data += \"        prov:agent  :OwnerAgent ;\\n\"\n",
    "            ttl_data += \"        dcat:hadRole :owner ;\\n\"\n",
    "            ttl_data += \"    ] ;\\n\"\n",
    "\n",
    "            # Store owner details separately to append at the end\n",
    "            owner_details = \"\\n:OwnerAgent a prov:Agent ;\\n\"\n",
    "\n",
    "            # Handle multiple names\n",
    "            names = value.get(\"name\")\n",
    "            if names:\n",
    "                if isinstance(names, str):  # Convert single name to a list for uniform processing\n",
    "                    names = [names]\n",
    "                for name in names:\n",
    "                    owner_details += \"     foaf:name \\\"{}\\\"^^xsd:string ;\\n\".format(fix_name(name))\n",
    "\n",
    "            # Handle email\n",
    "            email = fix_email(value.get(\"email\", \"\"))\n",
    "            if email:\n",
    "                owner_details += \"     foaf:mbox <mailto:{}> ; \\n\".format(email)\n",
    "            owner_details += \" .\\n\\n\"\n",
    "            # Ensure correct syntax: If no names or emails exist, don't write this block\n",
    "            if \"foaf:name\" not in owner_details and \"foaf:mbox\" not in owner_details:\n",
    "                owner_details = \"\"  # Remove empty block\n",
    "\n",
    "        elif key == \"website\":\n",
    "            ttl_data += f\"    foaf:page <{value}> ;\\n\"\n",
    "        elif key == \"triples\":\n",
    "            cleaned_integer = clean_integer(value)\n",
    "            ttl_data += f\"    void:triples \\\"{cleaned_integer}\\\"^^xsd:integer ;\\n\"          \n",
    "            # ttl_data += f\"    void:triples \\\"{value}\\\" ;\\n\"\n",
    "        elif key == \"license\":\n",
    "            ttl_data += f\"    dct:license <{value}> ;\\n\"\n",
    "        elif key == \"namespace\":\n",
    "            value = fix_name(value)\n",
    "            ttl_data += f\"    void:uriSpace <{value}> ;\\n\"\n",
    "        elif key == \"doi\":\n",
    "            ttl_data += f\"    dct:identifier \\\"{value}\\\" ;\\n\"\n",
    "        elif key == \"domain\":\n",
    "            value = fix_name(value)\n",
    "            ttl_data += f\"    dcat:theme \\\"{value}\\\" ;\\n\"\n",
    "        elif key == \"keywords\" and isinstance(value, list):  # Fix for keywords      \n",
    "            ttl_data += \"    dcat:keyword \" + \", \".join(f\"\\\"{fix_name(kw)}\\\"\" for kw in value) + \" ;\\n\"\n",
    "        elif key == \"image\":\n",
    "            ttl_data += f\"    foaf:depiction <{value}> ;\\n\"\n",
    "    \n",
    "    # Remove trailing semicolon for the last predicate\n",
    "    ttl_data = ttl_data.rstrip(\" ;\\n\") + \" .\\n\\n\"\n",
    "     \n",
    "    # Process Links Separately (Appending at the end)\n",
    "    if \"links\" in dataset and isinstance(dataset[\"links\"], list):\n",
    "        linkset_data += \"@prefix xsd: <http://www.w3.org/2001/XMLSchema#> . \\n\"\n",
    "        for item in dataset[\"links\"]:\n",
    "            if isinstance(item, dict) and \"target\" in item:\n",
    "                target = fix_name(item[\"target\"])\n",
    "                linkset_data += f\":{target} a void:Linkset ;\\n\"\n",
    "                linkset_data += f\"    void:target :{sanitized_id} ;\\n\"\n",
    "                if \"value\" in item:\n",
    "                    cleaned_integer = clean_integer(item[\"value\"])\n",
    "                    linkset_data += f\"    void:triples \\\"{cleaned_integer}\\\"^^xsd:integer ;\\n\"\n",
    "                linkset_data += \" .\\n\\n\"\n",
    "\n",
    "    return ttl_data + linkset_data + owner_details+ contact_details  # Append linkset data at the end\n",
    "\n",
    "\n",
    "def sanitize_filename(dataset_id):\n",
    "    \"\"\"Sanitize dataset ID to a valid filename.\"\"\"\n",
    "    return re.sub(r'[^a-zA-Z0-9_-]', '_', dataset_id)\n",
    "\n",
    "# Read the JSON file\n",
    "input_file = \"lod.json\"\n",
    "with open(input_file, \"r\") as f:\n",
    "    data = json.load(f)  # Load the JSON data\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"ttl_files\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Convert each dataset to TTL and save\n",
    "for dataset_id in data:\n",
    "    sanitized_id = sanitize_filename(dataset_id)\n",
    "    ttl_content = prefixes + \"\\n\" + json_to_ttl(data, dataset_id, sanitized_id)\n",
    "    ttl_output_path = os.path.join(output_dir, f\"{sanitized_id}.ttl\")\n",
    "    with open(ttl_output_path, \"w\") as file:\n",
    "        file.write(ttl_content)\n",
    "        \n",
    "def is_valid_email(email):\n",
    "    \"\"\"Helper function to check if the email is valid.\"\"\"\n",
    "    if not isinstance(email, str):\n",
    "        return False  # If the email is not a string, it's invalid\n",
    "    # Simple regex to check if the email has a valid format\n",
    "    pattern = r\"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\"\n",
    "    return re.match(pattern, email) is not None\n",
    "\n",
    "\n",
    "print(f\"TTL files saved in the folder: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c931924-f64c-456a-8cdc-858b6eededd5",
   "metadata": {},
   "source": [
    "# run shacl against turtle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3841e980-0846-46e2-a569-751b7bc452f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation complete. 0 TTL files were validated successfully.\n",
      "1 TTL files could not be validated.\n"
     ]
    }
   ],
   "source": [
    "import pyshacl\n",
    "import rdflib\n",
    "import os\n",
    "\n",
    "# Path to the SHACL file\n",
    "shacl_file_path = \"LOD_v4_mary-dct_shacl-Copy1.ttl\"  # Your SHACL file here\n",
    "\n",
    "# Directory containing the TTL files\n",
    "ttl_dir_path = \"ttl_files2\"  # Your TTL files directory\n",
    "\n",
    "# Output report file\n",
    "output_txt = \"output_report_test_shacl.txt\"\n",
    "\n",
    "valid_count = 0\n",
    "invalid_count = 0\n",
    "\n",
    "with open(output_txt, \"w\") as result_file:\n",
    "    for ttl_file in os.listdir(ttl_dir_path):\n",
    "        if ttl_file.endswith(\".ttl\"):\n",
    "            ttl_file_path = os.path.join(ttl_dir_path, ttl_file)\n",
    "\n",
    "            try:\n",
    "                # Load the TTL file into a graph\n",
    "                data_graph = rdflib.Graph()\n",
    "                data_graph.parse(ttl_file_path, format=\"turtle\")\n",
    "\n",
    "                # Create a SHACL graph\n",
    "                shapes_graph = rdflib.Graph()\n",
    "                shapes_graph.parse(shacl_file_path, format=\"turtle\")\n",
    "\n",
    "                # Validate using pyshacl\n",
    "                results = pyshacl.validate(\n",
    "                    data_graph,\n",
    "                    shacl_graph=shapes_graph,\n",
    "                    data_graph_format=\"ttl\",\n",
    "                    shacl_graph_format=\"ttl\",\n",
    "                    inference=\"rdfs\",\n",
    "                    debug=False,\n",
    "                    serialize_report_graph=\"ttl\",\n",
    "                )\n",
    "\n",
    "                conforms, report_graph, report_text = results\n",
    "\n",
    "                if conforms:\n",
    "                    valid_count += 1\n",
    "                    result_file.write(f\"{ttl_file} - VALID\\n\")\n",
    "                else:\n",
    "                    invalid_count += 1\n",
    "                    result_file.write(f\"{ttl_file} - INVALID\\n\")\n",
    "                    # Parse the report graph for violations\n",
    "                    report_graph = rdflib.Graph().parse(data=report_graph, format=\"turtle\")\n",
    "                    violation_count = 0\n",
    "                    for s in report_graph.subjects(predicate=rdflib.RDF.type, object=rdflib.URIRef(\"http://www.w3.org/ns/shacl#ValidationResult\")):\n",
    "                        violation_count += 1\n",
    "                        focus_node = report_graph.value(subject=s, predicate=rdflib.URIRef(\"http://www.w3.org/ns/shacl#focusNode\"))\n",
    "                        result_message = report_graph.value(subject=s, predicate=rdflib.URIRef(\"http://www.w3.org/ns/shacl#resultMessage\"))\n",
    "                        result_path = report_graph.value(subject=s, predicate=rdflib.URIRef(\"http://www.w3.org/ns/shacl#resultPath\"))\n",
    "                        result_file.write(f\"  Violation {violation_count}:\\n\")\n",
    "                        result_file.write(f\"    Focus Node: {focus_node}\\n\")\n",
    "                        result_file.write(f\"    Message: {result_message}\\n\")\n",
    "                        if result_path:\n",
    "                            result_file.write(f\"    Result Path: {result_path}\\n\")\n",
    "\n",
    "                    result_file.write(f\"  Total Violations: {violation_count}\\n\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                result_file.write(f\"{ttl_file} - ERROR: {e}\\n\")\n",
    "                invalid_count += 1\n",
    "\n",
    "# Print the final validation summary\n",
    "print(f\"Validation complete. {valid_count} TTL files were validated successfully.\")\n",
    "print(f\"{invalid_count} TTL files could not be validated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed47dd11-95a9-495b-be77-1024a552a087",
   "metadata": {},
   "source": [
    "# Show the violation report for 17 elements of the LOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "805695d3-5ae2-4b7f-80a9-a46fd245a227",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Errors Summary:\n",
      "dct:title (http://purl.org/dc/terms/title): 8 errors\n",
      "dct:description (http://purl.org/dc/terms/description): 14 errors\n",
      "foaf:page (http://xmlns.com/foaf/0.1/page): 318 errors\n",
      "void:triples (http://rdfs.org/ns/void#triples): 0 errors\n",
      "dct:license (http://purl.org/dc/terms/license): 738 errors\n",
      "void:uriSpace (http://rdfs.org/ns/void#uriSpace): 0 errors\n",
      "dct:identifier (http://purl.org/dc/terms/identifier): 0 errors\n",
      "foaf:depiction (http://xmlns.com/foaf/0.1/depiction): 0 errors\n",
      "dcat:keyword (http://www.w3.org/ns/dcat#keyword): 80 errors\n",
      "dcat:theme (http://www.w3.org/ns/dcat#theme): 0 errors\n",
      "void:sparqlEndpoint (http://rdfs.org/ns/void#sparqlEndpoint): 0 errors\n",
      "dcat:mediaType (http://www.w3.org/ns/dcat#mediaType): 1 errors\n",
      "dcat:downloadURL (http://www.w3.org/ns/dcat#downloadURL): 3489 errors\n",
      "adms:status (http://www.w3.org/ns/adms#status): 0 errors\n",
      "dcat:accessURL (http://www.w3.org/ns/dcat#accessURL): 522 errors\n",
      "dcat:endpointURL (http://www.w3.org/ns/dcat#endpointURL): 0 errors\n",
      "dcat:endpointDescription (http://www.w3.org/ns/dcat#endpointDescription): 0 errors\n",
      "dcat:distribution (http://www.w3.org/ns/dcat#distribution): 4347 errors\n",
      "void:exampleResource (http://rdfs.org/ns/void#exampleResource): 0 errors\n",
      "dct:mirror (http://purl.org/dc/terms/mirror): 0 errors\n",
      "prov:qualifiedAttribution (http://www.w3.org/ns/prov#qualifiedAttribution): 893 errors\n",
      "prov:agent (http://www.w3.org/ns/prov#agent): 893 errors\n",
      "foaf:name (http://xmlns.com/foaf/0.1/name): 793 errors\n",
      "foaf:mbox (http://xmlns.com/foaf/0.1/mbox): 993 errors\n",
      "void:Linkset (http://rdfs.org/ns/void#Linkset): 0 errors\n",
      "void:target (http://rdfs.org/ns/void#target): 0 errors\n",
      "\n",
      "Statistics:\n",
      "Total Errors: 13089\n",
      "Average Errors per Vocabulary: 503.42\n",
      "Maximum Errors in a Vocabulary: 4347\n",
      "Minimum Errors in a Vocabulary: 0\n",
      "Median Errors: 0.5\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "# List of vocabularies from the table with full URLs\n",
    "vocabularies = {\n",
    "    'dct:title': 'http://purl.org/dc/terms/title',\n",
    "    'dct:description': 'http://purl.org/dc/terms/description',\n",
    "    'foaf:page': 'http://xmlns.com/foaf/0.1/page',\n",
    "    'void:triples': 'http://rdfs.org/ns/void#triples',\n",
    "    'dct:license': 'http://purl.org/dc/terms/license',\n",
    "    'void:uriSpace': 'http://rdfs.org/ns/void#uriSpace',\n",
    "    'dct:identifier': 'http://purl.org/dc/terms/identifier',\n",
    "    'foaf:depiction': 'http://xmlns.com/foaf/0.1/depiction',\n",
    "    'dcat:keyword': 'http://www.w3.org/ns/dcat#keyword',\n",
    "    'dcat:theme': 'http://www.w3.org/ns/dcat#theme',\n",
    "    'void:sparqlEndpoint': 'http://rdfs.org/ns/void#sparqlEndpoint',       \n",
    "    'dcat:mediaType': 'http://www.w3.org/ns/dcat#mediaType',\n",
    "    'dcat:downloadURL': 'http://www.w3.org/ns/dcat#downloadURL',\n",
    "    'adms:status': 'http://www.w3.org/ns/adms#status',\n",
    "    'dcat:accessURL': 'http://www.w3.org/ns/dcat#accessURL',\n",
    "    'dcat:endpointURL': 'http://www.w3.org/ns/dcat#endpointURL',\n",
    "    'dcat:endpointDescription': 'http://www.w3.org/ns/dcat#endpointDescription',\n",
    "    'dcat:distribution': 'http://www.w3.org/ns/dcat#distribution',\n",
    "    'void:exampleResource': 'http://rdfs.org/ns/void#exampleResource',\n",
    "    'dct:mirror': 'http://purl.org/dc/terms/mirror',\n",
    "    'prov:qualifiedAttribution': 'http://www.w3.org/ns/prov#qualifiedAttribution',\n",
    "    'prov:agent': 'http://www.w3.org/ns/prov#agent',\n",
    "    'foaf:name': 'http://xmlns.com/foaf/0.1/name',\n",
    "    'foaf:mbox': 'http://xmlns.com/foaf/0.1/mbox',\n",
    "    'void:Linkset': 'http://rdfs.org/ns/void#Linkset',\n",
    "    'void:target': 'http://rdfs.org/ns/void#target'\n",
    "}\n",
    "\n",
    "# Path to the 'output_report_new2.txt' file\n",
    "file_path = 'output_report_new7.txt'\n",
    "\n",
    "# Initialize a dictionary to store the errors per vocabulary\n",
    "vocabulary_errors = {vocab: 0 for vocab in vocabularies}\n",
    "\n",
    "# Open the file and process it\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Look for the vocabularies in each line after the first colon\n",
    "        if 'Result Path:' in line:\n",
    "            result_path = line.split(':', 1)[-1].strip()\n",
    "            for vocab, url in vocabularies.items():\n",
    "                if url in result_path:\n",
    "                    vocabulary_errors[vocab] += 1\n",
    "\n",
    "# Calculate statistics\n",
    "total_errors = sum(vocabulary_errors.values())\n",
    "average_errors = total_errors / len(vocabularies) if len(vocabularies) > 0 else 0\n",
    "max_errors = max(vocabulary_errors.values())\n",
    "min_errors = min(vocabulary_errors.values())\n",
    "median_errors = statistics.median(vocabulary_errors.values())\n",
    "\n",
    "# Output results\n",
    "print(\"Vocabulary Errors Summary:\")\n",
    "for vocab, errors in vocabulary_errors.items():\n",
    "    print(f\"{vocab} ({vocabularies[vocab]}): {errors} errors\")\n",
    "\n",
    "print(\"\\nStatistics:\")\n",
    "print(f\"Total Errors: {total_errors}\")\n",
    "print(f\"Average Errors per Vocabulary: {average_errors:.2f}\")\n",
    "print(f\"Maximum Errors in a Vocabulary: {max_errors}\")\n",
    "print(f\"Minimum Errors in a Vocabulary: {min_errors}\")\n",
    "print(f\"Median Errors: {median_errors}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308d1fbe-14bf-4e85-a8cc-5bda46a419fb",
   "metadata": {},
   "source": [
    "# show the total number of key-value pairs in LOD json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a6256a7a-043e-44d3-93df-4b16d0d63528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Field Occurrences ---\n",
      "Title: 1573 occurrences\n",
      "Description: 1573 occurrences\n",
      "Full Download: 282 occurrences\n",
      "Other Download: 1090 occurrences\n",
      "SPARQL: 657 occurrences\n",
      "Example: 828 occurrences\n",
      "Keywords: 1493 occurrences\n",
      "Domain: 1244 occurrences\n",
      "Owner: 298 occurrences\n",
      "Website: 1255 occurrences\n",
      "Triples: 1568 occurrences\n",
      "License: 835 occurrences\n",
      "Namespace: 705 occurrences\n",
      "DOI: 17 occurrences\n",
      "Image URL: 67 occurrences\n",
      "Links: 1295 occurrences\n",
      "Contact point: 1573 occurrences\n",
      "\n",
      "--- Value Counts ---\n",
      "Title: 1573 total values\n",
      "Description: 1456 total values\n",
      "Full Download: 512 total values\n",
      "Other Download: 3544 total values\n",
      "SPARQL: 668 total values\n",
      "Example: 1173 total values\n",
      "Keywords: 15745 total values\n",
      "Domain: 1244 total values\n",
      "Owner: 298 total values\n",
      "Website: 1255 total values\n",
      "Triples: 1568 total values\n",
      "License: 835 total values\n",
      "Namespace: 705 total values\n",
      "DOI: 17 total values\n",
      "Image URL: 67 total values\n",
      "Links: 17882 total values\n",
      "Contact point: 1326 total values\n",
      "\n",
      "--- Subelement Counts ---\n",
      "Description:\n",
      "  en: 1456 occurrences\n",
      "Full Download:\n",
      "  media_type: 511 occurrences\n",
      "  description: 415 occurrences\n",
      "  status: 512 occurrences\n",
      "  title: 449 occurrences\n",
      "  download_url: 511 occurrences\n",
      "  mirror: 47 occurrences\n",
      "  _id: 267 occurrences\n",
      "SPARQL:\n",
      "  title: 451 occurrences\n",
      "  access_url: 668 occurrences\n",
      "  status: 668 occurrences\n",
      "  _id: 139 occurrences\n",
      "  description: 480 occurrences\n",
      "Example:\n",
      "  title: 607 occurrences\n",
      "  media_type: 1136 occurrences\n",
      "  access_url: 1173 occurrences\n",
      "  status: 1173 occurrences\n",
      "  description: 956 occurrences\n",
      "  _id: 109 occurrences\n",
      "Links:\n",
      "  target: 17882 occurrences\n",
      "  value: 17880 occurrences\n",
      "  _id: 219 occurrences\n",
      "Contact point:\n",
      "  name: 1323 occurrences\n",
      "  email: 953 occurrences\n",
      "Owner:\n",
      "  email: 290 occurrences\n",
      "  name: 20 occurrences\n",
      "Other Download:\n",
      "  media_type: 3464 occurrences\n",
      "  description: 2776 occurrences\n",
      "  status: 3544 occurrences\n",
      "  access_url: 3544 occurrences\n",
      "  title: 2179 occurrences\n",
      "  mirror: 661 occurrences\n",
      "  _id: 62 occurrences\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the LOD fields from the image\n",
    "lod_fields = [\n",
    "    \"Title\", \"Description\", \"Full Download\", \"Other Download\", \"SPARQL\", \"Example\",\n",
    "    \"Keywords\", \"Domain\", \"Owner\", \"Website\", \"Triples\", \"License\", \"Namespace\",\n",
    "    \"DOI\", \"Image URL\", \"Links\", \"Contact point\"\n",
    "]\n",
    "\n",
    "# Mapping JSON keys to LOD fields\n",
    "json_key_mapping = {\n",
    "    \"title\": \"Title\",\n",
    "    \"description\": \"Description\",\n",
    "    \"full_download\": \"Full Download\",\n",
    "    \"other_download\": \"Other Download\",\n",
    "    \"sparql\": \"SPARQL\",\n",
    "    \"example\": \"Example\",\n",
    "    \"keywords\": \"Keywords\",\n",
    "    \"domain\": \"Domain\",\n",
    "    \"owner\": \"Owner\",\n",
    "    \"website\": \"Website\",\n",
    "    \"triples\": \"Triples\",\n",
    "    \"license\": \"License\",\n",
    "    \"namespace\": \"Namespace\",\n",
    "    \"doi\": \"DOI\",\n",
    "    \"image\": \"Image URL\",\n",
    "    \"links\": \"Links\",\n",
    "    \"contact_point\": \"Contact point\"\n",
    "}\n",
    "\n",
    "# Load JSON file\n",
    "file_path = \"lod.json\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Initialize counters\n",
    "field_counts = defaultdict(int)\n",
    "value_counts = defaultdict(int)\n",
    "subelement_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Function to check if a dictionary has at least one non-empty value\n",
    "def has_non_empty_value(d):\n",
    "    if isinstance(d, dict):\n",
    "        return any(v not in [None, \"\", [], {}] for v in d.values())  # At least one valid value\n",
    "    return False\n",
    "\n",
    "# Count occurrences and values\n",
    "for dataset in data.values():\n",
    "    for key, lod_field in json_key_mapping.items():\n",
    "        if key in dataset:\n",
    "            value = dataset[key]\n",
    "            \n",
    "            # Only count occurrences if the value is NOT empty\n",
    "            if value not in [None, \"\", [], {}]:  \n",
    "                field_counts[lod_field] += 1\n",
    "            \n",
    "            # Count values in lists\n",
    "            if isinstance(value, list):\n",
    "                value_counts[lod_field] += len(value)\n",
    "                \n",
    "                # Count subelements for list of dictionaries\n",
    "                for item in value:\n",
    "                    if isinstance(item, dict):\n",
    "                        for subkey, subvalue in item.items():\n",
    "                            if subvalue not in [None, \"\", [], {}]:\n",
    "                                subelement_counts[lod_field][subkey] += 1\n",
    "            # Count dictionary as a value **ONLY if it has at least one non-empty value**\n",
    "            elif isinstance(value, dict) and has_non_empty_value(value):\n",
    "                value_counts[lod_field] += 1\n",
    "                \n",
    "                # Count subelements\n",
    "                for subkey, subvalue in value.items():\n",
    "                    if subvalue not in [None, \"\", [], {}]:\n",
    "                        subelement_counts[lod_field][subkey] += 1\n",
    "            # Count single values (strings, numbers) only if they are not empty\n",
    "            elif isinstance(value, (str, int, float)) and value != \"\":\n",
    "                value_counts[lod_field] += 1\n",
    "\n",
    "# Print the results\n",
    "print(\"\\n--- Field Occurrences ---\")\n",
    "for field in lod_fields:\n",
    "    print(f\"{field}: {field_counts.get(field, 0)} occurrences\")\n",
    "\n",
    "print(\"\\n--- Value Counts ---\")\n",
    "for field in lod_fields:\n",
    "    print(f\"{field}: {value_counts.get(field, 0)} total values\")\n",
    "\n",
    "print(\"\\n--- Subelement Counts ---\")\n",
    "for field, subfields in subelement_counts.items():\n",
    "    print(f\"{field}:\")\n",
    "    for subkey, count in subfields.items():\n",
    "        print(f\"  {subkey}: {count} occurrences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8c851e3-7bc8-445e-81a1-01248842e8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Field Occurrences (Number of datasets that have this field) ---\n",
      "Title: 1573 datasets\n",
      "Description: 1573 datasets\n",
      "Full Download: 282 datasets\n",
      "Other Download: 1090 datasets\n",
      "SPARQL: 657 datasets\n",
      "Example: 828 datasets\n",
      "Keywords: 1493 datasets\n",
      "Domain: 1244 datasets\n",
      "Owner: 298 datasets\n",
      "Website: 1255 datasets\n",
      "Triples: 1568 datasets\n",
      "License: 835 datasets\n",
      "Namespace: 705 datasets\n",
      "DOI: 17 datasets\n",
      "Image URL: 67 datasets\n",
      "Links: 1295 datasets\n",
      "Contact point: 1573 datasets\n",
      "\n",
      "--- Value Counts (Total instances of this field) ---\n",
      "Title: 1573 values\n",
      "Description: 1456 values\n",
      "Full Download: 512 values\n",
      "Other Download: 3544 values\n",
      "SPARQL: 668 values\n",
      "Example: 1173 values\n",
      "Keywords: 15743 values\n",
      "Domain: 1244 values\n",
      "Owner: 318 values\n",
      "Website: 1255 values\n",
      "Triples: 1568 values\n",
      "License: 835 values\n",
      "Namespace: 705 values\n",
      "DOI: 17 values\n",
      "Image URL: 67 values\n",
      "Links: 17882 values\n",
      "Contact point: 2276 values\n",
      "\n",
      "--- Subelement Counts (Details of nested elements) ---\n",
      "Description:\n",
      "  en: 1456 occurrences\n",
      "Full Download:\n",
      "  media_type: 511 occurrences\n",
      "  description: 415 occurrences\n",
      "  status: 512 occurrences\n",
      "  title: 449 occurrences\n",
      "  download_url: 511 occurrences\n",
      "  mirror: 47 occurrences\n",
      "  _id: 267 occurrences\n",
      "SPARQL:\n",
      "  title: 451 occurrences\n",
      "  access_url: 668 occurrences\n",
      "  status: 668 occurrences\n",
      "  _id: 139 occurrences\n",
      "  description: 480 occurrences\n",
      "Example:\n",
      "  title: 607 occurrences\n",
      "  media_type: 1136 occurrences\n",
      "  access_url: 1173 occurrences\n",
      "  status: 1173 occurrences\n",
      "  description: 956 occurrences\n",
      "  _id: 109 occurrences\n",
      "Links:\n",
      "  target: 17882 occurrences\n",
      "  value: 17880 occurrences\n",
      "  _id: 219 occurrences\n",
      "Contact point:\n",
      "  name: 1323 occurrences\n",
      "  email: 953 occurrences\n",
      "Owner:\n",
      "  email: 290 occurrences\n",
      "  name: 20 occurrences\n",
      "Other Download:\n",
      "  media_type: 3464 occurrences\n",
      "  description: 2776 occurrences\n",
      "  status: 3544 occurrences\n",
      "  access_url: 3544 occurrences\n",
      "  title: 2179 occurrences\n",
      "  mirror: 661 occurrences\n",
      "  _id: 62 occurrences\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the LOD fields from the image\n",
    "lod_fields = [\n",
    "    \"Title\", \"Description\", \"Full Download\", \"Other Download\", \"SPARQL\", \"Example\",\n",
    "    \"Keywords\", \"Domain\", \"Owner\", \"Website\", \"Triples\", \"License\", \"Namespace\",\n",
    "    \"DOI\", \"Image URL\", \"Links\", \"Contact point\"\n",
    "]\n",
    "\n",
    "# Mapping JSON keys to LOD fields\n",
    "json_key_mapping = {\n",
    "    \"title\": \"Title\",\n",
    "    \"description\": \"Description\",\n",
    "    \"full_download\": \"Full Download\",\n",
    "    \"other_download\": \"Other Download\",\n",
    "    \"sparql\": \"SPARQL\",\n",
    "    \"example\": \"Example\",\n",
    "    \"keywords\": \"Keywords\",\n",
    "    \"domain\": \"Domain\",\n",
    "    \"owner\": \"Owner\",\n",
    "    \"website\": \"Website\",\n",
    "    \"triples\": \"Triples\",\n",
    "    \"license\": \"License\",\n",
    "    \"namespace\": \"Namespace\",\n",
    "    \"doi\": \"DOI\",\n",
    "    \"image\": \"Image URL\",\n",
    "    \"links\": \"Links\",\n",
    "    \"contact_point\": \"Contact point\"\n",
    "}\n",
    "\n",
    "# Load JSON file\n",
    "file_path = \"lod.json\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Initialize counters\n",
    "field_counts = defaultdict(int)  # Counts datasets with a non-empty field\n",
    "value_counts = defaultdict(int)  # Counts the actual values inside fields\n",
    "subelement_counts = defaultdict(lambda: defaultdict(int))  # Tracks nested fields\n",
    "\n",
    "# Function to check if a dictionary has at least one non-empty value\n",
    "def has_non_empty_value(d):\n",
    "    if isinstance(d, dict):\n",
    "        return any(v not in [None, \"\", [], {}] for v in d.values())\n",
    "    return False\n",
    "\n",
    "# Count occurrences and values\n",
    "for dataset in data.values():\n",
    "    for key, lod_field in json_key_mapping.items():\n",
    "        if key in dataset:\n",
    "            value = dataset[key]\n",
    "\n",
    "            # Ensure we only count non-empty fields in field_counts\n",
    "            if value not in [None, \"\", [], {}]:  \n",
    "                field_counts[lod_field] += 1\n",
    "\n",
    "            # Handle lists properly\n",
    "            if isinstance(value, list):\n",
    "                non_empty_values = [item for item in value if item not in [None, \"\", [], {}]]\n",
    "                value_counts[lod_field] += len(non_empty_values)\n",
    "\n",
    "                # Count subelements in lists of dictionaries\n",
    "                for item in non_empty_values:\n",
    "                    if isinstance(item, dict):\n",
    "                        for subkey, subvalue in item.items():\n",
    "                            if subvalue not in [None, \"\", [], {}]:\n",
    "                                subelement_counts[lod_field][subkey] += 1\n",
    "            \n",
    "            # Handle dictionaries correctly\n",
    "            elif isinstance(value, dict) and has_non_empty_value(value):\n",
    "                non_empty_subfields = sum(1 for subkey, subvalue in value.items() if subvalue not in [None, \"\", [], {}])\n",
    "                value_counts[lod_field] += non_empty_subfields if non_empty_subfields > 0 else 1\n",
    "\n",
    "                # Count subelements\n",
    "                for subkey, subvalue in value.items():\n",
    "                    if subvalue not in [None, \"\", [], {}]:\n",
    "                        subelement_counts[lod_field][subkey] += 1\n",
    "\n",
    "            # Handle single values (strings, numbers)\n",
    "            elif isinstance(value, (str, int, float)) and value != \"\":\n",
    "                value_counts[lod_field] += 1\n",
    "\n",
    "# Print the results\n",
    "print(\"\\n--- Field Occurrences (Number of datasets that have this field) ---\")\n",
    "for field in lod_fields:\n",
    "    print(f\"{field}: {field_counts.get(field, 0)} datasets\")\n",
    "\n",
    "print(\"\\n--- Value Counts (Total instances of this field) ---\")\n",
    "for field in lod_fields:\n",
    "    print(f\"{field}: {value_counts.get(field, 0)} values\")\n",
    "\n",
    "print(\"\\n--- Subelement Counts (Details of nested elements) ---\")\n",
    "for field, subfields in subelement_counts.items():\n",
    "    print(f\"{field}:\")\n",
    "    for subkey, count in subfields.items():\n",
    "        print(f\"  {subkey}: {count} occurrences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc6ce4c-5b22-42fb-ba78-c751a87caa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "Full Download:\n",
    "  media_type: 511 occurrences\n",
    "  description: 415 occurrences\n",
    "  status: 512 occurrences\n",
    "  title: 449 occurrences\n",
    "  download_url: 511 occurrences\n",
    "  mirror: 47 occurrences\n",
    "  _id: 267 occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f5a12b-73ab-49a4-aedb-5e290cfa5e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e535c3d5-f541-4bf1-9f76-a048e427e6ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5d9975-5ffe-4526-9738-2511ab26a942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771979ea-28d6-43ed-af25-81387aa2f668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b3aec3-85f7-4ba2-babf-0346199d8ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bafdf2-3617-4501-9f3d-f07ed207683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl_files/wikilinks-rdf-nif.ttl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcd924e-56f6-4489-bad1-4c8cd68a0b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8256cb9f-7b01-4bd2-92f0-c68120e0c212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837433d8-e200-4719-bbea-3fa2ffbcb14f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba3f875-d07f-4421-96b6-9b505903826b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97398016-fd82-4c22-b95f-0e77f4540eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a46076-55e0-4eb3-96ee-148804b5e03a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c04f3e7-050a-42dc-a7b0-d50acc51da99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
