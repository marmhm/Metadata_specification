{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abdb7081-e31c-40f8-81e2-eb6117cdeff0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTL files saved in the folder: ttl_files\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re  # Import regex module for sanitizing file names\n",
    "\n",
    "# Load the initial JSON file\n",
    "file_path = \"lod-data.json\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Step 1: Replace spaces with hyphens in dataset IDs\n",
    "data = {key.replace(\" \", \"-\"): value for key, value in data.items()}\n",
    "\n",
    "# Step 2: Define fields to remove\n",
    "fields_to_remove = {\n",
    "    \"full_download\": [\"title\", \"description\", \"status\", \"mirror\", \"media_type\", \"_id\"],\n",
    "    \"sparql\": [\"_id\", \"title\", \"description\", \"status\"],\n",
    "    \"other_download\": [\"title\", \"description\", \"media_type\", \"status\", \"mirror\", \"_id\"],\n",
    "    \"example\": [\"title\", \"description\", \"media_type\", \"status\"],\n",
    "    \"links\": [\"value\"]\n",
    "}\n",
    "\n",
    "def remove_fields(obj, fields_to_remove):\n",
    "    \"\"\"Recursively remove specified fields from the JSON object.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: remove_fields(v, fields_to_remove) for k, v in obj.items() if k not in fields_to_remove}\n",
    "    elif isinstance(obj, list):\n",
    "        return [remove_fields(item, fields_to_remove) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Process each main element in the data\n",
    "for key, element in data.items():\n",
    "    for field, subfields in fields_to_remove.items():\n",
    "        if field in element:\n",
    "            element[field] = remove_fields(element[field], subfields)\n",
    "\n",
    "# Step 3: Flatten nested fields\n",
    "def flatten_fields_in_place(element):\n",
    "    \"\"\"Flatten nested fields in place for all fields.\"\"\"\n",
    "    for key, value in element.items():\n",
    "        if isinstance(value, list) and all(isinstance(item, dict) for item in value):\n",
    "            # Flatten a list of dictionaries to a list of values from all keys\n",
    "            flattened = []\n",
    "            for item in value:\n",
    "                if isinstance(item, dict):\n",
    "                    flattened.extend(item.values())\n",
    "            element[key] = flattened\n",
    "        elif isinstance(value, dict):\n",
    "            # Replace the field with its values if it's a dictionary\n",
    "            flattened = list(value.values())\n",
    "            # Use the first value if there's only one, otherwise keep the list\n",
    "            element[key] = flattened[0] if len(flattened) == 1 else flattened\n",
    "\n",
    "# Adjust the JSON structure for all fields\n",
    "for key, element in data.items():\n",
    "    if isinstance(element, dict):\n",
    "        flatten_fields_in_place(element)\n",
    "\n",
    "# Save the intermediate processed JSON\n",
    "intermediate_json_path = \"lod-data-processed.json\"\n",
    "with open(intermediate_json_path, \"w\") as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "# Step 4: Convert JSON to TTL\n",
    "prefixes = \"\"\"\n",
    "@prefix : <http://example.org/> . \n",
    "@prefix dct: <http://purl.org/dc/terms/> . \n",
    "@prefix void: <http://rdfs.org/ns/void#> . \n",
    "@prefix dcat: <http://www.w3.org/ns/dcat#> . \n",
    "@prefix foaf: <http://xmlns.com/foaf/0.1/> . \n",
    "@prefix schema: <http://schema.org/> . \n",
    "\"\"\"\n",
    "\n",
    "def json_to_ttl(json_data, dataset_id):\n",
    "    ttl_data = f\":{dataset_id} a dcat:Dataset ;\\n\"\n",
    "    dataset = json_data[dataset_id]\n",
    "    for key, value in dataset.items():\n",
    "        if not value:  # Skip keys with empty or None values\n",
    "            continue\n",
    "        if key == \"title\":\n",
    "            ttl_data += f\"    dct:title \\\"{value}\\\" ;\\n\"\n",
    "        elif key == \"description\":\n",
    "            ttl_data += f\"    dct:description \\\"\\\"\\\"{value}\\\"\\\"\\\" ;\\n\"  # Multiline string\n",
    "        elif key == \"full_download\" and isinstance(value, list):\n",
    "            ttl_data += \"    dct:distribution \" + \", \".join(f'<{kw}>' for kw in value) + \" ;\\n\"\n",
    "        elif key == \"sparql\" and isinstance(value, list):\n",
    "            ttl_data += \"    void:sparqlEndpoint \" + \", \".join(f'<{kw}>' for kw in value) + \" ;\\n\"\n",
    "        elif key == \"other_download\" and isinstance(value, list):\n",
    "            ttl_data += \"    dct:distribution \" + \", \".join(f'<{kw}>' for kw in value) + \" ;\\n\"\n",
    "        elif key == \"example\" and isinstance(value, list):\n",
    "            ttl_data += \"    void:exampleResource \" + \", \".join(f'\"{item}\"' for item in value) + \" ;\\n\"\n",
    "        elif key == \"keywords\" and isinstance(value, list):\n",
    "            ttl_data += \"    dcat:keyword \" + \", \".join(f'\"{kw}\"' for kw in value) + \" ;\\n\"\n",
    "        elif key == \"owner\" and isinstance(value, list):\n",
    "            ttl_data += \"    dct:creator \" + \", \".join(f'\"{item}\"' for item in value) + \" ;\\n\"\n",
    "        elif key == \"website\":\n",
    "            ttl_data += f\"    foaf:page <{value}> ;\\n\"\n",
    "        elif key == \"triples\":\n",
    "            ttl_data += f\"    void:triples \\\"{value}\\\" ;\\n\"\n",
    "        elif key == \"license\":\n",
    "            ttl_data += f\"    dct:license <{value}> ;\\n\"\n",
    "        elif key == \"namespace\":\n",
    "            ttl_data += f\"    void:uriSpace <{value}> ;\\n\"\n",
    "        elif key == \"doi\":\n",
    "            ttl_data += f\"    dct:identifier \\\"{value}\\\" ;\\n\"\n",
    "        elif key == \"contact_point\" and isinstance(value, list):\n",
    "            ttl_data += \"    dcat:contactPoint \" + \", \".join(f'\"{item}\"' for item in value) + \" ;\\n\"\n",
    "        elif key == \"domain\":\n",
    "            ttl_data += f\"    dcat:keyword \\\"{value}\\\" ;\\n\"\n",
    "        elif key == \"image\":\n",
    "            ttl_data += f\"    foaf:depiction <{value}> ;\\n\"\n",
    "        elif key == \"links\" and isinstance(value, list):\n",
    "            ttl_data += \"    void:target \" + \", \".join(f'\"{item}\"' for item in value) + \" ;\\n\"\n",
    "    # Remove trailing semicolon for the last predicate\n",
    "    ttl_data = ttl_data.rstrip(\" ;\\n\") + \" .\\n\\n\"\n",
    "    return ttl_data\n",
    "\n",
    "# Save each dataset in a separate TTL file\n",
    "output_dir = \"ttl_files\"  # Define the folder to save TTL files\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "def sanitize_filename(dataset_id):\n",
    "    # Replace any non-alphanumeric characters (including ':', '/', etc.) with underscores\n",
    "    return re.sub(r'[^a-zA-Z0-9_-]', '_', dataset_id)\n",
    "\n",
    "for dataset_id in data:\n",
    "    sanitized_id = sanitize_filename(dataset_id)  # Sanitize dataset ID to a valid filename\n",
    "    ttl_content = prefixes + \"\\n\" + json_to_ttl(data, dataset_id)\n",
    "    ttl_output_path = os.path.join(output_dir, f\"{sanitized_id}.ttl\")  # Path for the TTL file\n",
    "    with open(ttl_output_path, \"w\") as file:\n",
    "        file.write(ttl_content)\n",
    "\n",
    "print(f\"TTL files saved in the folder: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecdc613e-907c-4cb6-a781-dd90f80ae119",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation complete. 0 TTL files were validated successfully.\n",
      "1573 TTL files could not be validated.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import rdflib\n",
    "import pyshacl\n",
    "\n",
    "# Load the initial JSON file\n",
    "file_path = \"lod-data.json\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Step 1: Replace spaces with hyphens in dataset IDs\n",
    "data = {key.replace(\" \", \"-\"): value for key, value in data.items()}\n",
    "\n",
    "# Step 2: Define fields to remove\n",
    "fields_to_remove = {\n",
    "    \"full_download\": [\"title\", \"description\", \"status\", \"mirror\", \"media_type\", \"_id\"],\n",
    "    \"sparql\": [\"_id\", \"title\", \"description\", \"status\"],\n",
    "    \"other_download\": [\"title\", \"description\", \"media_type\", \"status\", \"mirror\", \"_id\"],\n",
    "    \"example\": [\"title\", \"description\", \"media_type\", \"status\"],\n",
    "    \"links\": [\"value\"]\n",
    "}\n",
    "\n",
    "def remove_fields(obj, fields_to_remove):\n",
    "    \"\"\"Recursively remove specified fields from the JSON object.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: remove_fields(v, fields_to_remove) for k, v in obj.items() if k not in fields_to_remove}\n",
    "    elif isinstance(obj, list):\n",
    "        return [remove_fields(item, fields_to_remove) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Process each main element in the data\n",
    "for key, element in data.items():\n",
    "    for field, subfields in fields_to_remove.items():\n",
    "        if field in element:\n",
    "            element[field] = remove_fields(element[field], subfields)\n",
    "\n",
    "# Step 3: Flatten nested fields\n",
    "def flatten_fields_in_place(element):\n",
    "    \"\"\"Flatten nested fields in place for all fields.\"\"\"\n",
    "    for key, value in element.items():\n",
    "        if isinstance(value, list) and all(isinstance(item, dict) for item in value):\n",
    "            # Flatten a list of dictionaries to a list of values from all keys\n",
    "            flattened = []\n",
    "            for item in value:\n",
    "                if isinstance(item, dict):\n",
    "                    flattened.extend(item.values())\n",
    "            element[key] = flattened\n",
    "        elif isinstance(value, dict):\n",
    "            # Replace the field with its values if it's a dictionary\n",
    "            flattened = list(value.values())\n",
    "            # Use the first value if there's only one, otherwise keep the list\n",
    "            element[key] = flattened[0] if len(flattened) == 1 else flattened\n",
    "\n",
    "# Adjust the JSON structure for all fields\n",
    "for key, element in data.items():\n",
    "    if isinstance(element, dict):\n",
    "        flatten_fields_in_place(element)\n",
    "\n",
    "# Save the intermediate processed JSON\n",
    "intermediate_json_path = \"lod-data-processed.json\"\n",
    "with open(intermediate_json_path, \"w\") as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "# Step 4: Convert JSON to TTL\n",
    "prefixes = \"\"\"\n",
    "@prefix : <http://example.org/> . \n",
    "@prefix dct: <http://purl.org/dc/terms/> . \n",
    "@prefix void: <http://rdfs.org/ns/void#> . \n",
    "@prefix dcat: <http://www.w3.org/ns/dcat#> . \n",
    "@prefix foaf: <http://xmlns.com/foaf/0.1/> . \n",
    "@prefix schema: <http://schema.org/> . \n",
    "\"\"\"\n",
    "\n",
    "def json_to_ttl(json_data, dataset_id):\n",
    "    ttl_data = f\":{dataset_id} a dcat:Dataset ;\\n\"\n",
    "    dataset = json_data[dataset_id]\n",
    "    for key, value in dataset.items():\n",
    "        if not value:  # Skip keys with empty or None values\n",
    "            continue\n",
    "        if key == \"title\":\n",
    "            ttl_data += f\"    dct:title \\\"{value}\\\" ;\\n\"\n",
    "        elif key == \"description\":\n",
    "            ttl_data += f\"    dct:description \\\"\\\"\\\"{value}\\\"\\\"\\\" ;\\n\"  # Multiline string\n",
    "        elif key == \"full_download\" and isinstance(value, list):\n",
    "            ttl_data += \"    dct:distribution \" + \", \".join(f'\"{kw}\"' for kw in value) + \" ;\\n\"\n",
    "        elif key == \"sparql\" and isinstance(value, list):\n",
    "            ttl_data += \"    void:sparqlEndpoint \" + \", \".join(f'\"{kw}\"' for kw in value) + \" ;\\n\"\n",
    "        elif key == \"other_download\" and isinstance(value, list):\n",
    "            ttl_data += \"    dct:distribution \" + \", \".join(f'\"{kw}\"' for kw in value) + \" ;\\n\"\n",
    "        elif key == \"example\" and isinstance(value, list):\n",
    "            ttl_data += \"    void:exampleResource \" + \", \".join(f'\"{item}\"' for item in value) + \" ;\\n\"\n",
    "        elif key == \"keywords\" and isinstance(value, list):\n",
    "            ttl_data += \"    dcat:keyword \" + \", \".join(f'\"{kw}\"' for kw in value) + \" ;\\n\"\n",
    "        elif key == \"owner\" and isinstance(value, list):\n",
    "            ttl_data += \"    dct:creator \" + \", \".join(f'\"{item}\"' for item in value) + \" ;\\n\"\n",
    "        elif key == \"website\":\n",
    "            ttl_data += f\"    foaf:page \\\"{value}\\\" ;\\n\"\n",
    "        elif key == \"triples\":\n",
    "            ttl_data += f\"    void:triples \\\"{value}\\\" ;\\n\"\n",
    "        elif key == \"license\":\n",
    "            ttl_data += f\"    dct:license \\\"{value}\\\" ;\\n\"\n",
    "        elif key == \"namespace\":\n",
    "            ttl_data += f\"    void:uriSpace \\\"{value}\\\" ;\\n\"\n",
    "        elif key == \"doi\":\n",
    "            ttl_data += f\"    dct:identifier \\\"{value}\\\" ;\\n\"\n",
    "        elif key == \"contact_point\" and isinstance(value, list):\n",
    "            ttl_data += \"    dcat:contactPoint \" + \", \".join(f'\"{item}\"' for item in value) + \" ;\\n\"\n",
    "        elif key == \"domain\":\n",
    "            ttl_data += f\"    dcat:keyword \\\"{value}\\\" ;\\n\"\n",
    "        elif key == \"image\":\n",
    "            ttl_data += f\"    foaf:depiction \\\"{value}\\\" ;\\n\"\n",
    "        elif key == \"links\" and isinstance(value, list):\n",
    "            ttl_data += \"    void:target \" + \", \".join(f'\"{item}\"' for item in value) + \" ;\\n\"\n",
    "    # Remove trailing semicolon for the last predicate\n",
    "    ttl_data = ttl_data.rstrip(\" ;\\n\") + \" .\\n\\n\"\n",
    "    return ttl_data\n",
    "\n",
    "# Save each dataset in a separate TTL file\n",
    "output_dir = \"ttl_files\"  # Define the folder to save TTL files\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "def sanitize_filename(dataset_id):\n",
    "    # Replace any non-alphanumeric characters (including ':', '/', etc.) with underscores\n",
    "    return re.sub(r'[^a-zA-Z0-9_-]', '_', dataset_id)\n",
    "\n",
    "for dataset_id in data:\n",
    "    sanitized_id = sanitize_filename(dataset_id)  # Sanitize dataset ID to a valid filename\n",
    "    ttl_content = prefixes + \"\\n\" + json_to_ttl(data, dataset_id)\n",
    "    ttl_output_path = os.path.join(output_dir, f\"{sanitized_id}.ttl\")  # Path for the TTL file\n",
    "    with open(ttl_output_path, \"w\") as file:\n",
    "        file.write(ttl_content)\n",
    "\n",
    "# Step 5: Validate each TTL file using SHACL\n",
    "output_txt = \"output_report.txt\"\n",
    "shacl_file_path = \"mary-dct_shacl.ttl\"  # Specify your SHACL file here\n",
    "\n",
    "valid_count = 0\n",
    "invalid_count = 0\n",
    "\n",
    "with open(output_txt, \"w\") as result_file:\n",
    "    for ttl_file in os.listdir(output_dir):\n",
    "        if ttl_file.endswith(\".ttl\"):\n",
    "            ttl_file_path = os.path.join(output_dir, ttl_file)\n",
    "            \n",
    "            try:\n",
    "                data_graph = rdflib.Graph()\n",
    "                data_graph.parse(ttl_file_path, format=\"turtle\")\n",
    "\n",
    "                # Create a SHACL graph\n",
    "                shapes_graph = rdflib.Graph()\n",
    "                shapes_graph.parse(shacl_file_path, format=\"turtle\")\n",
    "\n",
    "                results = pyshacl.validate(\n",
    "                    data_graph,\n",
    "                    shacl_graph=shapes_graph,\n",
    "                    data_graph_format=\"ttl\",\n",
    "                    shacl_graph_format=\"ttl\",\n",
    "                    inference=\"rdfs\",\n",
    "                    debug=False,\n",
    "                    serialize_report_graph=\"ttl\",\n",
    "                )\n",
    "\n",
    "                conforms, report_graph, report_text = results\n",
    "\n",
    "                if conforms:\n",
    "                    valid_count += 1\n",
    "                    result_file.write(f\"{ttl_file} - VALID\\n\")\n",
    "                else:\n",
    "                    invalid_count += 1\n",
    "                    result_file.write(f\"{ttl_file} - INVALID\\n\")\n",
    "                    # You can add more detailed violation info if needed\n",
    "                    report_graph = rdflib.Graph().parse(data=report_graph, format=\"turtle\")\n",
    "                    for s in report_graph.subjects(predicate=rdflib.RDF.type, object=rdflib.URIRef(\"http://www.w3.org/ns/shacl#ValidationResult\")):\n",
    "                        violation_message = report_graph.value(subject=s, predicate=rdflib.URIRef(\"http://www.w3.org/ns/shacl#resultMessage\"))\n",
    "                        result_file.write(f\"  Violation: {violation_message}\\n\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                result_file.write(f\"{ttl_file} - ERROR: {e}\\n\")\n",
    "                invalid_count += 1\n",
    "\n",
    "# Print the final validation summary\n",
    "print(f\"Validation complete. {valid_count} TTL files were validated successfully.\")\n",
    "print(f\"{invalid_count} TTL files could not be validated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fccafecb-3a5a-4232-894b-13763d1f9f48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation complete. 0 TTL files were validated successfully.\n",
      "1573 TTL files could not be validated.\n"
     ]
    }
   ],
   "source": [
    "import pyshacl\n",
    "import rdflib\n",
    "import os\n",
    "\n",
    "# Path to the SHACL file\n",
    "shacl_file_path = \"mary-dct_shacl.ttl\"  # Your SHACL file here\n",
    "\n",
    "# Directory containing the TTL files\n",
    "ttl_dir_path = \"ttl_files\"  # Your TTL files directory\n",
    "\n",
    "# Output report file\n",
    "output_txt = \"output_report.txt\"\n",
    "\n",
    "valid_count = 0\n",
    "invalid_count = 0\n",
    "\n",
    "with open(output_txt, \"w\") as result_file:\n",
    "    for ttl_file in os.listdir(ttl_dir_path):\n",
    "        if ttl_file.endswith(\".ttl\"):\n",
    "            ttl_file_path = os.path.join(ttl_dir_path, ttl_file)\n",
    "\n",
    "            try:\n",
    "                # Load the TTL file into a graph\n",
    "                data_graph = rdflib.Graph()\n",
    "                data_graph.parse(ttl_file_path, format=\"turtle\")\n",
    "\n",
    "                # Create a SHACL graph\n",
    "                shapes_graph = rdflib.Graph()\n",
    "                shapes_graph.parse(shacl_file_path, format=\"turtle\")\n",
    "\n",
    "                # Validate using pyshacl\n",
    "                results = pyshacl.validate(\n",
    "                    data_graph,\n",
    "                    shacl_graph=shapes_graph,\n",
    "                    data_graph_format=\"ttl\",\n",
    "                    shacl_graph_format=\"ttl\",\n",
    "                    inference=\"rdfs\",\n",
    "                    debug=False,\n",
    "                    serialize_report_graph=\"ttl\",\n",
    "                )\n",
    "\n",
    "                conforms, report_graph, report_text = results\n",
    "\n",
    "                if conforms:\n",
    "                    valid_count += 1\n",
    "                    result_file.write(f\"{ttl_file} - VALID\\n\")\n",
    "                else:\n",
    "                    invalid_count += 1\n",
    "                    result_file.write(f\"{ttl_file} - INVALID\\n\")\n",
    "                    # Parse the report graph for violations\n",
    "                    report_graph = rdflib.Graph().parse(data=report_graph, format=\"turtle\")\n",
    "                    violation_count = 0\n",
    "                    for s in report_graph.subjects(predicate=rdflib.RDF.type, object=rdflib.URIRef(\"http://www.w3.org/ns/shacl#ValidationResult\")):\n",
    "                        violation_count += 1\n",
    "                        focus_node = report_graph.value(subject=s, predicate=rdflib.URIRef(\"http://www.w3.org/ns/shacl#focusNode\"))\n",
    "                        result_message = report_graph.value(subject=s, predicate=rdflib.URIRef(\"http://www.w3.org/ns/shacl#resultMessage\"))\n",
    "                        result_path = report_graph.value(subject=s, predicate=rdflib.URIRef(\"http://www.w3.org/ns/shacl#resultPath\"))\n",
    "                        result_file.write(f\"  Violation {violation_count}:\\n\")\n",
    "                        result_file.write(f\"    Focus Node: {focus_node}\\n\")\n",
    "                        result_file.write(f\"    Message: {result_message}\\n\")\n",
    "                        if result_path:\n",
    "                            result_file.write(f\"    Result Path: {result_path}\\n\")\n",
    "\n",
    "                    result_file.write(f\"  Total Violations: {violation_count}\\n\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                result_file.write(f\"{ttl_file} - ERROR: {e}\\n\")\n",
    "                invalid_count += 1\n",
    "\n",
    "# Print the final validation summary\n",
    "print(f\"Validation complete. {valid_count} TTL files were validated successfully.\")\n",
    "print(f\"{invalid_count} TTL files could not be validated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "992e055f-9260-4bcb-887a-a9b1c0874036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Total Datasets': 1536,\n",
       " 'Total Violations': 16227,\n",
       " 'Average Violations per Dataset': 10.564453125,\n",
       " 'Max Violations in a Single Dataset': 25,\n",
       " 'Min Violations in a Single Dataset': 8}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open and analyze the provided file to count the datasets and summarize the content\n",
    "file_path = 'output_report.txt'\n",
    "\n",
    "# Initialize counters and structures for analysis\n",
    "total_datasets = 0\n",
    "violation_summary = []\n",
    "\n",
    "# Reading the file content\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Identify dataset lines and increment the counter\n",
    "        if line.strip().endswith('- INVALID'):\n",
    "            total_datasets += 1\n",
    "        # Identify total violations for each dataset\n",
    "        if line.strip().startswith('Total Violations:'):\n",
    "            violations = int(line.strip().split(':')[-1].strip())\n",
    "            violation_summary.append(violations)\n",
    "\n",
    "# Compile results\n",
    "summary = {\n",
    "    'Total Datasets': total_datasets,\n",
    "    'Total Violations': sum(violation_summary),\n",
    "    'Average Violations per Dataset': sum(violation_summary) / total_datasets if total_datasets > 0 else 0,\n",
    "    'Max Violations in a Single Dataset': max(violation_summary) if violation_summary else 0,\n",
    "    'Min Violations in a Single Dataset': min(violation_summary) if violation_summary else 0\n",
    "}\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822c60c0-2735-44ce-950a-66fcd1ef5ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
